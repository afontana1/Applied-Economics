{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a neural network with loss $J = f(Wx +b) = f(y)$ where $f \\in \\mathbf{R}^{m}\\rightarrow \\mathbf{R}$, $W\\in\\mathbf{R}^{m\\times d}$ and $b\\in \\mathbf{R}^d$ we need to compute the gradient of $J$ with respect to $W$, $x$, and $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, compute the partial derivative with respect to a single element of $W$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial W_{ij}} &= \\frac{\\partial J}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{ij}}\\\\\n",
    "&= \\frac{\\partial J}{\\partial y_i} \\frac{\\partial }{\\partial W_{ij}}\\left(\\sum_{k=1}^d W_{ik}x_k + b_k\\right)\\\\\n",
    "&= \\frac{\\partial J}{\\partial y_i} \\left(\\sum_{k=1}^d \\delta_{ik}x_k\\right)\\\\\n",
    "&= \\frac{\\partial J}{\\partial y_i} x_j\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $J$ with respect to $W$ is simply given by \n",
    "\\begin{align*}\n",
    "\\nabla_W J = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial y_1} x_1 & \\frac{\\partial J}{\\partial y_1} x_2 & \\cdots & \\frac{\\partial J}{\\partial y_1} x_d \\\\\n",
    "\\frac{\\partial J}{\\partial y_2} x_1 & \\frac{\\partial J}{\\partial y_2} x_2 & \\cdots & \\frac{\\partial J}{\\partial y_2} x_d \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial J}{\\partial y_m} x_1 & \\frac{\\partial J}{\\partial y_m} x_2 & \\cdots & \\frac{\\partial J}{\\partial y_m} x_d \\\\\n",
    "\\end{bmatrix}\\\\\n",
    " \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial y_1}\\\\\n",
    "\\frac{\\partial J}{\\partial y_2}\\\\\n",
    "\\frac{\\partial J}{\\partial y_3}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial J}{\\partial y_m}\\\\\n",
    "\\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 \\cdots x_d\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the partial derivative w.r.t $b$ can be done by first observing the derivative for a single element\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial b_i} &= \\frac{\\partial J}{\\partial y_i}\\frac{\\partial y_i}{\\partial b_i}\\\\\n",
    "&= \\frac{\\partial J}{\\partial y_i}\\cdot 1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of the loss wrt to the bias is \n",
    "\\begin{align*}\n",
    "\\nabla_b J = \\nabla J_y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient with respect to the input of the layer we can similarly look at the derivative wrt to a single element.\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial x_{i}} &= \\sum_{j=1}^m\\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_{i}}\\\\\n",
    "&=  \\sum_{j=1}^m\\frac{\\partial J}{\\partial y_j}\\frac{\\partial}{\\partial x_{i}} \\left(\\sum_{k=1}^d W_{jk}x_{k}+b_k\\right)\\\\\n",
    "&=  \\sum_{j=1}^m\\frac{\\partial J}{\\partial y_j}\\left(\\sum_{k=1}^d \\delta_{ik}W_{jk}\\right)\\\\\n",
    "&=  \\sum_{j=1}^m\\frac{\\partial J}{\\partial y_j}W_{ji}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the loss wrt to the input $x$ is then given by \n",
    "\\begin{align*}\n",
    "\\nabla_x J = W^T\\nabla_y J\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of an elementwise activation $\\sigma$ the loss is $J=f(\\sigma(A))$. The gradient with repsect to the matrix $A$ can be found looking at the partial derivative with respect to a single element of $A$. \n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial A_{ij}} &= \\frac{\\partial J}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial A_{ij}}\\\\\n",
    "&= \\frac{\\partial J}{\\partial y_{ij}} \\sigma'(A_{ij})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The gradient wrt to $A$ is then $\\nabla_A J = \\nabla_y J \\odot \\sigma'(A)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
