{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests & Boosting Webinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - ### Decision Trees\n",
    "    - ### Bootstrapping\n",
    "    - ### Bagging\n",
    "    - ### Random Forests\n",
    "    - ### Boosting\n",
    "- ### Examples:\n",
    "    - ### Random forests\n",
    "    - ### Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we use decision trees, random forests, & boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees make no assumptions about the distribution of the data (i.e. nonparametric)\n",
    "- Decision trees map regression and classification problems very well\n",
    "- The resulting model is clear for interpretability (e.g. important variables, model can be easily visualize for non-technical audience, etc.)\n",
    "- Random forests are a simple, clear extension of decisions trees to improve performance\n",
    "    - Using the bootstrap is one way it improves over a simple decision tree as the bootstrap often reduces bias in finite samples (i.e. asymptotic refinements)\n",
    "- Empirical applications have shown random forests and boosted decision trees to be effective out-of-the-box algorithms, especially with a non-linear function\n",
    "    - XGBoost is a very popular gradient boosted decision tree algorithm that has won many online data science competitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of concepts: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CarType](images/ex_tree_car_own.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RegTree](images/reg_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build a tree?\n",
    "\n",
    "Look at all the cut points, of all the variables, and decide which ones improves the algorithm the most.\n",
    "\n",
    "**Well, what is \"improves the algorithm the most\"?**\n",
    "\n",
    "### Decision rules\n",
    "\n",
    "- ### Classification error\n",
    "- ### Entropy\n",
    "- ### Information gain\n",
    "- ### Gini impurity\n",
    "\n",
    "### Why not grow a huge tree for minimal training error?\n",
    "\n",
    "What's the answer in 95% of machine learning questions?\n",
    "\n",
    "**AVOID OVER-FITTING!**\n",
    "\n",
    "\n",
    "![Overfit](images/overfit.png)\n",
    "\n",
    "\n",
    "There are loads of \"regularization\" methods to find minimum of test error by not over-fitting on training error.\n",
    "\n",
    "- Tree depth\n",
    "- Number of leaves\n",
    "- Number of nodes\n",
    "- Leaf size\n",
    "- Limit splits to above a certain classification error reduction\n",
    "- Pruning (i.e. total cost formula to find optimal tree complexity and training error)\n",
    "- Among many others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - ### Bootstrapping\n",
    "    - Bagging\n",
    "    - Random Forests\n",
    "    - Boosting\n",
    "- ### Examples:\n",
    "    - Random forests\n",
    "    - Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of concepts: Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The bootstrap is resampling from our data to estimate the distribution of an estimator.**\n",
    "\n",
    "![Bootstrap](images/bootstrap.png)\n",
    "\n",
    "- I do not follow the math for why the bootstrap actually gives improved finite sample performance -- termed asymptotic refinements -- (it seems like magic), but intuitively it makes some sense:\n",
    "    - given our random sample, each observation had an equal probability of arising\n",
    "    - thus, when we take a random sample of our random sample, our data still have an equal probability of ending up in the bootstrap random sample\n",
    "    - So, it is as if we are able to conduct many experiments but we are working with a smaller data set than the population; but, hopefully because these data were collected that they should be over weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - Bootstrapping\n",
    "    - ### Bagging\n",
    "    - Random Forests\n",
    "    - Boosting\n",
    "- ### Examples:\n",
    "    - Random forests\n",
    "    - Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of concepts: Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging is Bootstrap AGGregation where we create many bootstrap samples, fit models on all, and then combine.**\n",
    "\n",
    "![Bagging](images/bagging.png)\n",
    "\n",
    "- A good analogy is it is the wisdom of the crowds: each model doesn't need to be great but their average is highly predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - Bootstrapping\n",
    "    - Bagging\n",
    "    - ### Random Forests\n",
    "    - Boosting\n",
    "- ### Examples:\n",
    "    - Random forests\n",
    "    - Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of concepts: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest is an ensemble algorithm which creates many decision trees using bagging and dropping variables.**\n",
    "\n",
    "![RF_Algorithm](images/rf.png)\n",
    "\n",
    "\n",
    "We set m ~= sqrt(d), which is an empirical result as opposed to a mathematical result.\n",
    "\n",
    "Improvement from reducing the correlation between each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - Bootstrapping\n",
    "    - Bagging\n",
    "    - Random Forests\n",
    "    - ### Boosting\n",
    "- ### Examples:\n",
    "    - Random forests\n",
    "    - Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ensembling?\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model. For example, given predictions from several models we could:\n",
    "\n",
    "- **Regression:** Take the average of the predictions.\n",
    "- **Classification:** Take a vote and use the most common prediction.\n",
    "\n",
    "For ensembling to work well, the models must be:\n",
    "\n",
    "- **Accurate:** They outperform the null model.\n",
    "- **Independent:** Their predictions are generated using different processes.\n",
    "\n",
    "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when you average the models.\n",
    "\n",
    "There are two basic **methods for ensembling:**\n",
    "\n",
    "- Manually ensembling your individual models.\n",
    "- Using a model that ensembles for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Manual Ensembling With a Single Model Approach\n",
    "\n",
    "**Advantages of manual ensembling:**\n",
    "\n",
    "- It increases predictive accuracy.\n",
    "- It's easy to get started.\n",
    "\n",
    "**Disadvantages of manual ensembling:**\n",
    "\n",
    "- It decreases interpretability.\n",
    "- It takes longer to train.\n",
    "- It takes longer to predict.\n",
    "- It is more complex to automate and maintain.\n",
    "- Small gains in accuracy may not be worth the added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of concepts: Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted is an intelligent way of improving weakness in the model with each new bootstrap sample+model fit.\n",
    "\n",
    "![boosted](images/boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - Bootstrapping\n",
    "    - Bagging\n",
    "    - Random Forests\n",
    "    - Boosting\n",
    "- ### Examples:\n",
    "    - ### Random forests\n",
    "    - Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admissions data\n",
    "\n",
    "Classify [admissions] using predictors: [gre, gpa, prestige]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "  Using cached https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pydotplus) (2.3.0)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Running setup.py bdist_wheel for pydotplus: started\n",
      "  Running setup.py bdist_wheel for pydotplus: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\david\\AppData\\Local\\pip\\Cache\\wheels\\35\\7b\\ab\\66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "\n",
    "import pydotplus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load admissions data\n",
    "admit = pd.read_csv('datasets/admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the nulls instead of imputing right now, to save time\n",
    "admit = admit.dropna()\n",
    "admit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up the data set\n",
    "y_class = [-1 if k == 0 else k for k in admit['admit']]\n",
    "X_class = admit[['gpa','gre','prestige']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tts \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class,\n",
    "    test_size = .25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build tree\n",
    "model_dt = DecisionTreeClassifier(max_depth=2)\n",
    "model_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Classifier max depth 4:  [0.65333333 0.72972973 0.75675676 0.71621622] Mean score: 0.7140090090090091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt_scores = cross_val_score(model_dt, X_train, y_train, cv=4)\n",
    "print(\"Decision Classifier max depth 4: \", model_dt_scores, 'Mean score:', np.mean(model_dt_scores))\n",
    "model_dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Classifier max depth 4:  [0.66666667 0.74324324 0.75675676 0.72972973] Mean score: 0.7240990990990992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=20, max_features=2, max_depth=4, bootstrap=True)\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_rf_scores = cross_val_score(model_rf, X_train, y_train, cv=4)\n",
    "print(\"Decision Classifier max depth 4: \", model_rf_scores, 'Mean score:', np.mean(model_rf_scores))\n",
    "model_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the college dataset to see if we can get a similiar improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.read_csv('datasets/College.csv')\n",
    "y   = col.Private.map(lambda x: 1 if x == 'Yes' else -1)\n",
    "X   = col.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Apps  Accept  Enroll  Top10perc  Top25perc  F.Undergrad  P.Undergrad  \\\n",
       "0  1660    1232     721         23         52         2885          537   \n",
       "1  2186    1924     512         16         29         2683         1227   \n",
       "2  1428    1097     336         22         50         1036           99   \n",
       "3   417     349     137         60         89          510           63   \n",
       "4   193     146      55         16         44          249          869   \n",
       "\n",
       "   Outstate  Room.Board  Books  Personal  PhD  Terminal  S.F.Ratio  \\\n",
       "0      7440        3300    450      2200   70        78       18.1   \n",
       "1     12280        6450    750      1500   29        30       12.2   \n",
       "2     11250        3750    400      1165   53        66       12.9   \n",
       "3     12960        5450    450       875   92        97        7.7   \n",
       "4      7560        4120    800      1500   76        72       11.9   \n",
       "\n",
       "   perc.alumni  Expend  Grad.Rate  \n",
       "0           12    7041         60  \n",
       "1           16   10527         56  \n",
       "2           30    8735         54  \n",
       "3           37   19016         59  \n",
       "4            2   10922         15  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = .25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt = DecisionTreeClassifier(max_depth=5)\n",
    "model_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Classifier max depth 4:  [0.88435374 0.91724138 0.91724138 0.91724138] Mean score: 0.9090194698569082\n"
     ]
    }
   ],
   "source": [
    "model_dt_scores = cross_val_score(model_dt, X_train, y_train, cv=4)\n",
    "print(\"Decision Classifier max depth 4: \",model_dt_scores, 'Mean score:', np.mean(model_dt_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128205128205128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier max depth  5 :  [0.93197279 0.93793103 0.91724138 0.94482759] Mean score: 0.9329931972789115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9384615384615385"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth_input = 5\n",
    "model_rf = RandomForestClassifier(n_estimators=100, max_features=6, max_depth=max_depth_input, bootstrap=True)\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_rf_scores = cross_val_score(model_rf, X_train, y_train, cv=4)\n",
    "print(\"Random Forest Classifier max depth \",max_depth_input, \": \", model_rf_scores, 'Mean score:', np.mean(model_rf_scores))\n",
    "model_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- ### Why do we use decision trees, random forests, & boosting\n",
    "- ### Review of concepts:\n",
    "    - Decision Trees\n",
    "    - Bootstrapping\n",
    "    - Bagging\n",
    "    - Random Forests\n",
    "    - Boosting\n",
    "- ### Examples:\n",
    "    - Random forests\n",
    "    - ### Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples: Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.read_csv('datasets/College.csv')\n",
    "y =col.Private.map(lambda x: 1 if x == 'Yes' else -1)\n",
    "X = col.iloc[:, 2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = .25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958974358974359"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bst = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_features=6, max_depth=5), n_estimators=200, learning_rate=0.01, random_state=1)\n",
    "model_bst.fit(X_train, y_train)\n",
    "model_bst.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRAS!\n",
    "\n",
    "## Let's see how gradient boosting does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.read_csv('datasets/College.csv')\n",
    "y   = col.Private.map(lambda x: 1 if x == 'Yes' else -1)\n",
    "X   = col.iloc[:, 2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9435897435897436"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grad_bst = GradientBoostingClassifier(max_features=8, max_depth=5, n_estimators=400, learning_rate=0.05, random_state=1)\n",
    "model_grad_bst.fit(X_train, y_train)\n",
    "model_grad_bst.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the admissions data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit = pd.read_csv('datasets/admissions.csv')\n",
    "admit = admit.dropna()\n",
    "y = [-1 if k == 0 else k for k in admit['admit']]\n",
    "X = admit[['gpa','gre','prestige']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad Boost Classifier:  [0.74666667 0.70666667 0.66216216 0.73972603] Mean score: 0.7138053807231889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grad_bst = GradientBoostingClassifier(max_features=2, max_depth=2, n_estimators=2750, learning_rate=0.001, random_state=1)\n",
    "model_grad_bst.fit(X_train, y_train)\n",
    "model_grad_bst_scores = cross_val_score(model_grad_bst, X_train, y_train, cv=4)\n",
    "print(\"Grad Boost Classifier: \",model_grad_bst_scores, 'Mean score:', np.mean(model_grad_bst_scores))\n",
    "model_grad_bst.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care about balanced classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the college private/public data again, but let's make it imbalanced\n",
    "col = pd.read_csv('datasets/College.csv')\n",
    "col_priv = col.loc[col['Private'] == 'Yes']\n",
    "col_pub = col.loc[col['Private'] == 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 19)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_priv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 19)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_pub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 19)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_priv = col_priv.sample(200)\n",
    "col_pub = col_pub.sample(200)\n",
    "col = col_priv.append(col_pub)\n",
    "col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y   = col.Private.map(lambda x: 1 if x == 'Yes' else -1)\n",
    "X   = col.iloc[:, 2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier max depth  5 :  [0.92105263 0.94736842 0.93243243 0.91891892] Mean score: 0.9299431009957325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth_input = 5\n",
    "model_rf = RandomForestClassifier(n_estimators=100, max_features=6, max_depth=max_depth_input, bootstrap=True)\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_rf_scores = cross_val_score(model_rf, X_train, y_train, cv=4)\n",
    "print(\"Random Forest Classifier max depth \",max_depth_input, \": \", model_rf_scores, 'Mean score:', np.mean(model_rf_scores))\n",
    "model_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grad_bst = GradientBoostingClassifier(max_features=2, max_depth=9, n_estimators=200, learning_rate=0.005, random_state=1)\n",
    "model_grad_bst.fit(X_train, y_train)\n",
    "model_grad_bst.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n",
    "- https://en.wikipedia.org/wiki/Random_forest\n",
    "- https://en.wikipedia.org/wiki/Gradient_boosting#Informal_introduction\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "- Joel Horowitz. 2001. The Bootstrap. Handbook of Econometrics. https://www.sciencedirect.com/science/article/pii/S157344120105005X?via%3Dihub\n",
    "- Elements of Stat. Learning by Hastie https://web.stanford.edu/~hastie/ElemStatLearn//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
