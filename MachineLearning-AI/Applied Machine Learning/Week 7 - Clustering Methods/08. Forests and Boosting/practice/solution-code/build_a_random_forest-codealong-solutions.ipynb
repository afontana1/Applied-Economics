{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A (Basic) Random Forest from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "> **Note**: this is intended to be a group work lab or a codealong with the instructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Random Forest?\n",
    "\n",
    "---\n",
    "\n",
    "Random Forests are some of the most widespread classifiers used. They are relatively simple to use because they require very few parameters to set and they perform well. As we have seen, Decision Trees are very powerful machine learning models.\n",
    "\n",
    "Decision Trees have some critical limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets. Bagging helps mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n",
    "\n",
    "Random forests are a further way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "### Feature bagging\n",
    "\n",
    "Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called feature bagging. \n",
    "\n",
    "The reason for doing this is due to correlation of trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the bagging base trees, causing them to become correlated. By selecting a random subset of the features at each split, we avoid this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "**For a problem with p features, it is typical to use:**\n",
    "- `p^(1/2)` (rounded down) features in each split for a classification problem.\n",
    "- `p/3` (rounded down) with a minimum node size of 5 as the default for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Instructions\n",
    "\n",
    "---\n",
    "\n",
    "**A Random Forest classifier is built satisfying these conditions:**\n",
    "1. Multiple internal decision tree classifiers will be built as the base models.\n",
    "- For each base model, the data will be resampled with replacement (bootstrapping).\n",
    "- Each decision tree will be fit on one of the bootstrapped samples of the original data.\n",
    "- To predict, each internal base model will be passed the new data and make their predictions. The final output will be a vote across the base models for the class.\n",
    "\n",
    "**Your custom random forest classifier must:**\n",
    "1. Accept hyperparameters `max_features`, `n_estimators`, and `max_depth`.\n",
    "2. Implement a `fit` method.\n",
    "3. Implement a `predict` method.\n",
    "4. Implement a `score` method.\n",
    "5. Satisfy the conditions for random forest classifiers listed above!\n",
    "6. **You will not be implementing feature bagging in this lab, for the sake of simplicity!**\n",
    "\n",
    "**Test your random forest classifier on the (pre-cleaned) Titanic dataset and compare the performance to a single decision tree classifier!**\n",
    "\n",
    "> *Note: you are allowed to use the `DecisionTreeClassifier` class in sklearn for the internal base estimators. This lab is about building the random forest ensemble estimator, not building a decision tree class!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../../data/titanic_clean.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-762cbe69d4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitanic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/titanic_clean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ricky.hennessy/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ricky.hennessy/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ricky.hennessy/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ricky.hennessy/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ricky.hennessy/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../../data/titanic_clean.csv' does not exist"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('../../data/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will be the custom, basic version of a Random Forest in the style of\n",
    "# sklearn's models\n",
    "class RandomForest(object):\n",
    "    \n",
    "    # The __init__ function takes 3 keyword arguments:\n",
    "    # n_estimators: how many decision tree classifiers are we going to \n",
    "    # fit?\n",
    "    # max_depth: what is the max depth of the internal \"base estimator\"\n",
    "    # decision trees?\n",
    "    # max_features: also a parameter passed to the decision tree classifier\n",
    "    def __init__(self, n_estimators=3, max_depth=None, max_features=None):\n",
    "        \n",
    "        # Set up the 3 keyword arguments as class attributes:\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        # create a dictionary that will hold our base estimators\n",
    "        self.base_estimators = {}\n",
    "        \n",
    "        \n",
    "    # The makes the internal decision tree classifiers\n",
    "    # Takes data X, y and also an estimator number for\n",
    "    # assignment to the self.base_estimators dictionary\n",
    "    def _make_base_estimator(self, X, y, estimator_number):\n",
    "        \n",
    "        # make random sample of X, y\n",
    "        row_indices = list(range(X.shape[0]))\n",
    "        \n",
    "        # np.random.choice gives us a random sample of the row\n",
    "        # indice with replacement\n",
    "        random_indices = np.random.choice(row_indices, size=len(row_indices),\n",
    "                                          replace=True)\n",
    "        \n",
    "        \n",
    "        # make \"bootstrapped\" X, y\n",
    "        # versions of the data indexed by the randomly sampled row indices\n",
    "        Xr = X.iloc[random_indices, :]\n",
    "        yr = y[random_indices]\n",
    "        \n",
    "        # Initialize a decision tree classifier with the max_depth\n",
    "        # and max_features attributes\n",
    "        dtc = DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                     max_features=self.max_features)\n",
    "        \n",
    "        # fit the dtc:\n",
    "        dtc.fit(Xr, yr)\n",
    "        \n",
    "        # put the dtc into our base_estimator dictionary\n",
    "        self.base_estimators[estimator_number] = dtc\n",
    "                \n",
    "    \n",
    "    # Fitting is actually delegated to the _make_base_estimator function.\n",
    "    # You specify a number of estimators that you want to fit, and so\n",
    "    # it iterates through that range, fitting that number of trees\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            self._make_base_estimator(X, y, i)\n",
    "            \n",
    "    \n",
    "    # Predict calls back into the stored base estimators to get\n",
    "    # predictions for each. \n",
    "    # NOTE: THIS FUNCTION ONLY WORKS FOR THE BINARY 1/0 CLASS\n",
    "    # PROBLEM!\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # set up votes array with just zeros in it, \n",
    "        # the length of rows in X:\n",
    "        votes = np.zeros(X.shape[0])\n",
    "        \n",
    "        # iterate through all the base estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # get out the current one from the dictionary\n",
    "            base_model = self.base_estimators[i]\n",
    "            \n",
    "            # predict with the base estimator on the X matrix\n",
    "            current_pred = base_model.predict(X)\n",
    "            \n",
    "            # add whatever the predictions are to the votes array\n",
    "            votes = votes + current_pred\n",
    "            \n",
    "        # The final output predictions will be 1 any time the vote \n",
    "        # count for class 1 was greater than half the number of \n",
    "        # base estimators. \n",
    "        yhat = (votes >= self.n_estimators/2.).astype(np.int)\n",
    "        \n",
    "        return yhat\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        y = np.array(y)\n",
    "        \n",
    "        yhat = self.predict(X)\n",
    "        \n",
    "        return ()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('Survived ~ C(Pclass) + C(Sex) + Age + SibSp + Parch + Fare + C(Embarked) -1',\n",
    "                      data=titanic, return_type='dataframe')\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4044943820224719"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtc acc: 0.753191489362\n",
      "rf acc: 0.8\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(n_estimators=1000, max_depth=None, max_features='auto')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=None, max_features='auto')\n",
    "dtc.fit(Xtrain, ytrain)\n",
    "print('dtc acc:', dtc.score(Xtest, ytest))\n",
    "\n",
    "rf.fit(Xtrain, ytrain)\n",
    "yhat = rf.predict(Xtest)\n",
    "print('rf acc:', accuracy_score(ytest, yhat))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
