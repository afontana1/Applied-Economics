{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A (Basic) Random Forest from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "> **Note**: this is intended to be a group work lab or a codealong with the instructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Random Forest?\n",
    "\n",
    "---\n",
    "\n",
    "Random Forests are some of the most widespread classifiers used. They are relatively simple to use because they require very few parameters to set and they perform well. As we have seen, Decision Trees are very powerful machine learning models.\n",
    "\n",
    "Decision Trees have some critical limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets. Bagging helps mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n",
    "\n",
    "Random forests are a further way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "### Feature bagging\n",
    "\n",
    "Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called feature bagging. \n",
    "\n",
    "The reason for doing this is due to correlation of trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the bagging base trees, causing them to become correlated. By selecting a random subset of the features at each split, we avoid this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "**For a problem with p features, it is typical to use:**\n",
    "- `p^(1/2)` (rounded down) features in each split for a classification problem.\n",
    "- `p/3` (rounded down) with a minimum node size of 5 as the default for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Instructions\n",
    "\n",
    "---\n",
    "\n",
    "**A Random Forest classifier is built satisfying these conditions:**\n",
    "1. Multiple internal decision tree classifiers will be built as the base models.\n",
    "- For each base model, the data will be resampled with replacement (bootstrapping).\n",
    "- Each decision tree will be fit on one of the bootstrapped samples of the original data.\n",
    "- To predict, each internal base model will be passed the new data and make their predictions. The final output will be a vote across the base models for the class.\n",
    "\n",
    "**Your custom random forest classifier must:**\n",
    "1. Accept hyperparameters `max_features`, `n_estimators`, and `max_depth`.\n",
    "2. Implement a `fit` method.\n",
    "3. Implement a `predict` method.\n",
    "4. Implement a `score` method.\n",
    "5. Satisfy the conditions for random forest classifiers listed above!\n",
    "6. **You will not be implementing feature bagging in this lab, for the sake of simplicity!**\n",
    "\n",
    "**Test your random forest classifier on the (pre-cleaned) Titanic dataset and compare the performance to a single decision tree classifier!**\n",
    "\n",
    "> *Note: you are allowed to use the `DecisionTreeClassifier` class in sklearn for the internal base estimators. This lab is about building the random forest ensemble estimator, not building a decision tree class!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-14b23177b4a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitanic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/titanic_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('../data/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will be the custom, basic version of a Random Forest in the style of\n",
    "# sklearn's models\n",
    "class RandomForest(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def score(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
